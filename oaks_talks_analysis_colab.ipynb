{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chapmjs/oaks_talks_analysis/blob/main/oaks_talks_analysis_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ua5noe7MzTc"
      },
      "source": [
        "# President Dallin H. Oaks - Complete Talk Collection Analysis",
        "\n",
        "This notebook analyzes ALL talks by President Dallin H. Oaks using the comprehensive collection from bencrowder.net.",
        "**Instructions**: Run each cell in order by clicking the play button or pressing Shift+Enter.",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/chapmjs/oaks_talks_analysis/blob/main/oaks_talks_analysis_colab.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h89p-CYlMzTd"
      },
      "source": [
        "## Step 1: Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBlRYgVCMzTf"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install beautifulsoup4 wordcloud nltk pandas matplotlib seaborn scikit-learn lxml -q\n",
        "\n",
        "# Download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "print(\"‚úì Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U81H3R_sMzTg"
      },
      "source": [
        "## Step 2: Create Project Structure and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp-OLdFtMzTh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import re\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from IPython.display import Image, display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('data/talks', exist_ok=True)\n",
        "os.makedirs('output/wordclouds', exist_ok=True)\n",
        "os.makedirs('output/analysis', exist_ok=True)\n",
        "\n",
        "print(\"‚úì Project structure created!\")\n",
        "print(\"\\nDirectories:\")\n",
        "print(\"  - data/talks/ (for downloaded talks)\")\n",
        "print(\"  - output/wordclouds/ (for word cloud images)\")\n",
        "print(\"  - output/analysis/ (for analysis reports)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o54Y3KbRMzTi"
      },
      "source": [
        "## Step 3: Define Stopwords and Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdf3PMSVMzTj"
      },
      "outputs": [],
      "source": [
        "# Custom stopwords for religious/conference texts\n",
        "ENGLISH_STOPWORDS = {\n",
        "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",\n",
        "    \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he',\n",
        "    'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\",\n",
        "    'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
        "    'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
        "    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
        "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',\n",
        "    'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
        "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
        "    'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
        "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'both',\n",
        "    'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
        "    'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will',\n",
        "    'just', 'should', 'now'\n",
        "}\n",
        "\n",
        "CONFERENCE_COMMON = {\n",
        "    'elder', 'president', 'brother', 'sister', 'saint', 'saints',\n",
        "    'conference', 'general', 'talk', 'spoke', 'speaking', 'said', 'says',\n",
        "    'today', 'time', 'year', 'years', 'day', 'days',\n",
        "    'may', 'might', 'must', 'shall', 'would', 'could', 'should',\n",
        "    'also', 'even', 'well', 'much', 'many', 'every',\n",
        "    'first', 'second', 'third', 'last', 'next',\n",
        "    'one', 'two', 'three', 'four', 'five',\n",
        "    'know', 'known', 'knew', 'think', 'thought',\n",
        "    'come', 'came', 'go', 'went', 'going',\n",
        "    'make', 'made', 'making', 'give', 'gave', 'given',\n",
        "    'see', 'saw', 'seen', 'say', 'said', 'saying',\n",
        "    'us', 'let', 'way', 'like', 'want', 'need'\n",
        "}\n",
        "\n",
        "# Additional stopwords specific to talks\n",
        "ADDITIONAL_STOPS = {\n",
        "    'lord', 'god', 'jesus', 'christ', 'church', 'lds',\n",
        "    'latter', 'day', 'will', 'can', 'also', 'one', 'two',\n",
        "    'brethren', 'sisters', 'oaks', 'dallin', 'president',\n",
        "    'thank', 'name', 'amen'\n",
        "}\n",
        "\n",
        "ALL_STOPWORDS = ENGLISH_STOPWORDS | CONFERENCE_COMMON | ADDITIONAL_STOPS\n",
        "\n",
        "print(f\"‚úì Loaded {len(ALL_STOPWORDS)} stopwords\")\n",
        "\n",
        "# Text processing class\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stopwords = ALL_STOPWORDS\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        text = re.sub(r'\\b\\d+\\b', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_talk_content(self, filepath):\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        if '\\n---\\n' in content:\n",
        "            content = content.split('\\n---\\n', 1)[1]\n",
        "        return content\n",
        "\n",
        "    def process_text(self, text, remove_stops=True):\n",
        "        text = self.clean_text(text)\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [t for t in tokens if t not in string.punctuation and any(c.isalpha() for c in t)]\n",
        "        if remove_stops:\n",
        "            tokens = [t for t in tokens if t.lower() not in self.stopwords]\n",
        "        return tokens\n",
        "\n",
        "processor = TextProcessor()\n",
        "print(\"‚úì Text processor initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-_JUkfXMzTj"
      },
      "source": [
        "## Step 4: Fetch Talks from bencrowder.net\\n**Note**: This may take 30-60 minutes on first run as it downloads hundreds of talks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9BjXKMqMzTk"
      },
      "outputs": [],
      "source": [
        "class TalkFetcher:\n",
        "    def __init__(self):\n",
        "        self.base_url = \"https://bencrowder.net\"\n",
        "        self.speaker_url = f\"{self.base_url}/collected-talks/dallin-h-oaks/\"\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        self.data_dir = \"data/talks\"\n",
        "\n",
        "    def get_talk_links(self):\n",
        "        print(\"Fetching talk list from bencrowder.net...\")\n",
        "        try:\n",
        "            response = requests.get(self.speaker_url, headers=self.headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            talks = []\n",
        "            talk_list = soup.find('div', class_='entry-content') or soup.find('article') or soup.find('main')\n",
        "\n",
        "            if talk_list:\n",
        "                links = talk_list.find_all('a')\n",
        "                for link in links:\n",
        "                    href = link.get('href', '')\n",
        "                    text = link.get_text(strip=True)\n",
        "\n",
        "                    if href and ('churchofjesuschrist.org' in href or 'speeches.byu.edu' in href or 'lds.org' in href):\n",
        "                        parent_text = link.parent.get_text() if link.parent else text\n",
        "                        date_match = re.search(r'(\\d{1,2}\\s+\\w+\\s+\\d{4}|\\w+\\s+\\d{4}|\\d{4})', parent_text)\n",
        "                        date = date_match.group(0) if date_match else 'Unknown'\n",
        "\n",
        "                        if 'general-conference' in href:\n",
        "                            talk_type = 'General Conference'\n",
        "                        elif 'byu' in href:\n",
        "                            talk_type = 'BYU Speech'\n",
        "                        else:\n",
        "                            talk_type = 'Other'\n",
        "\n",
        "                        talk_info = {\n",
        "                            'title': text,\n",
        "                            'url': href,\n",
        "                            'date': date,\n",
        "                            'type': talk_type\n",
        "                        }\n",
        "\n",
        "                        if not any(t['url'] == href for t in talks):\n",
        "                            talks.append(talk_info)\n",
        "\n",
        "            print(f\"Found {len(talks)} unique talks\")\n",
        "            return talks\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def fetch_talk_content(self, url):\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Try different selectors\n",
        "            selectors = ['article', 'div.body-block', 'div.article-content', 'div.transcript', 'main']\n",
        "            article = None\n",
        "            for selector in selectors:\n",
        "                article = soup.select_one(selector)\n",
        "                if article:\n",
        "                    break\n",
        "\n",
        "            if article:\n",
        "                for element in article(['script', 'style', 'nav', 'header', 'footer']):\n",
        "                    element.decompose()\n",
        "                text = article.get_text(separator=' ', strip=True)\n",
        "                return re.sub(r'\\s+', ' ', text)\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "    def save_talk(self, talk_info, content):\n",
        "        safe_title = re.sub(r'[^\\w\\s-]', '', talk_info['title'][:50])\n",
        "        safe_title = re.sub(r'[-\\s]+', '-', safe_title)\n",
        "        year_match = re.search(r'\\d{4}', talk_info['date'])\n",
        "        year = year_match.group(0) if year_match else 'unknown'\n",
        "        talk_type = talk_info['type'].replace(' ', '_')\n",
        "\n",
        "        filename = f\"{year}_{talk_type}_{safe_title}.txt\"\n",
        "        filepath = os.path.join(self.data_dir, filename)\n",
        "\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"Title: {talk_info['title']}\\n\")\n",
        "            f.write(f\"Type: {talk_info['type']}\\n\")\n",
        "            f.write(f\"Date: {talk_info['date']}\\n\")\n",
        "            f.write(f\"URL: {talk_info['url']}\\n\")\n",
        "            f.write(\"\\n---\\n\\n\")\n",
        "            f.write(content)\n",
        "\n",
        "        return filepath\n",
        "\n",
        "    def fetch_all_talks(self, limit=None):\n",
        "        talks = self.get_talk_links()\n",
        "        if not talks:\n",
        "            return 0\n",
        "\n",
        "        if limit:\n",
        "            talks = talks[:limit]\n",
        "            print(f\"Limiting to {limit} talks for demo purposes\")\n",
        "\n",
        "        successful = 0\n",
        "        failed = []\n",
        "\n",
        "        print(f\"\\nDownloading {len(talks)} talks...\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        for i, talk in enumerate(talks, 1):\n",
        "            print(f\"[{i}/{len(talks)}] {talk['title'][:60]}...\", end=' ')\n",
        "\n",
        "            # Check if already downloaded\n",
        "            year_match = re.search(r'\\d{4}', talk['date'])\n",
        "            year = year_match.group(0) if year_match else 'unknown'\n",
        "            safe_title = re.sub(r'[^\\w\\s-]', '', talk['title'][:50])\n",
        "            safe_title = re.sub(r'[-\\s]+', '-', safe_title)\n",
        "            talk_type = talk['type'].replace(' ', '_')\n",
        "            filename = f\"{year}_{talk_type}_{safe_title}.txt\"\n",
        "            filepath = os.path.join(self.data_dir, filename)\n",
        "\n",
        "            if os.path.exists(filepath):\n",
        "                print(\"‚úì (already downloaded)\")\n",
        "                successful += 1\n",
        "                continue\n",
        "\n",
        "            content = self.fetch_talk_content(talk['url'])\n",
        "            if content and len(content) > 100:\n",
        "                self.save_talk(talk, content)\n",
        "                print(\"‚úì\")\n",
        "                successful += 1\n",
        "            else:\n",
        "                print(\"‚úó\")\n",
        "                failed.append(talk['title'])\n",
        "\n",
        "            time.sleep(1.5)  # Rate limiting\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"Downloaded {successful} talks successfully\")\n",
        "        if failed:\n",
        "            print(f\"Failed: {len(failed)} talks\")\n",
        "\n",
        "        return successful\n",
        "\n",
        "# Fetch talks (limit to 50 for demo, remove limit for all talks)\n",
        "fetcher = TalkFetcher()\n",
        "num_talks = fetcher.fetch_all_talks(limit=50)  # Remove 'limit=50' to get all talks\n",
        "print(f\"\\n‚úì Ready to analyze {num_talks} talks!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDMaw0liMzTm"
      },
      "source": [
        "## Step 5: Generate Word Clouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2lw9PsRMzTm"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "# Get all talk files\n",
        "talk_files = glob.glob('data/talks/*.txt')\n",
        "print(f\"Found {len(talk_files)} talk files to analyze\\n\")\n",
        "\n",
        "# Combine all texts\n",
        "all_text = \"\"\n",
        "for filepath in talk_files:\n",
        "    content = processor.extract_talk_content(filepath)\n",
        "    all_text += \" \" + content\n",
        "\n",
        "# Generate main word cloud\n",
        "print(\"Generating main word cloud...\")\n",
        "wordcloud = WordCloud(\n",
        "    width=1600,\n",
        "    height=900,\n",
        "    background_color='white',\n",
        "    stopwords=ALL_STOPWORDS,\n",
        "    max_words=150,\n",
        "    colormap='viridis',\n",
        "    relative_scaling=0.5,\n",
        "    min_font_size=10\n",
        ").generate(all_text)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title('President Dallin H. Oaks - Complete Talk Collection', fontsize=24, pad=20)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('output/wordclouds/main_wordcloud.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Main word cloud saved to output/wordclouds/main_wordcloud.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4mUpxWZMzTn"
      },
      "source": [
        "## Step 6: Word Frequency Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-azAkJZvMzTn"
      },
      "outputs": [],
      "source": [
        "# Analyze word frequencies\n",
        "print(\"Analyzing word frequencies...\\n\")\n",
        "\n",
        "all_tokens = []\n",
        "for filepath in talk_files:\n",
        "    content = processor.extract_talk_content(filepath)\n",
        "    tokens = processor.process_text(content)\n",
        "    all_tokens.extend(tokens)\n",
        "\n",
        "# Get word frequencies\n",
        "word_freq = Counter(all_tokens)\n",
        "top_words = word_freq.most_common(30)\n",
        "\n",
        "# Create DataFrame\n",
        "df_freq = pd.DataFrame(top_words, columns=['Word', 'Frequency'])\n",
        "total_words = sum(word_freq.values())\n",
        "df_freq['Percentage'] = (df_freq['Frequency'] / total_words * 100).round(2)\n",
        "\n",
        "# Display top words\n",
        "print(\"Top 30 Most Frequent Words:\")\n",
        "print(\"=\"*50)\n",
        "for idx, row in df_freq.iterrows():\n",
        "    print(f\"{idx+1:3}. {row['Word']:20} {row['Frequency']:6,} ({row['Percentage']:.2f}%)\")\n",
        "\n",
        "# Save to CSV\n",
        "df_freq.to_csv('output/analysis/word_frequencies.csv', index=False)\n",
        "print(\"\\n‚úì Word frequencies saved to output/analysis/word_frequencies.csv\")\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(range(20), df_freq['Frequency'].head(20), color='steelblue')\n",
        "plt.xticks(range(20), df_freq['Word'].head(20), rotation=45, ha='right')\n",
        "plt.xlabel('Words', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Top 20 Most Frequent Words - President Dallin H. Oaks', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('output/analysis/word_frequency_chart.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBEpMiVKMzTn"
      },
      "source": [
        "## Step 7: Analyze by Decade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J0CpltOMzTn"
      },
      "outputs": [],
      "source": [
        "# Group talks by decade\n",
        "decades = defaultdict(list)\n",
        "\n",
        "for filepath in talk_files:\n",
        "    basename = os.path.basename(filepath)\n",
        "    year_match = re.match(r'^(\\d{4})', basename)\n",
        "\n",
        "    if year_match:\n",
        "        year = int(year_match.group(1))\n",
        "        decade = f\"{(year // 10) * 10}s\"\n",
        "        content = processor.extract_talk_content(filepath)\n",
        "        decades[decade].append(content)\n",
        "\n",
        "print(f\"Found talks from {len(decades)} decades:\")\n",
        "for decade in sorted(decades.keys()):\n",
        "    print(f\"  {decade}: {len(decades[decade])} talks\")\n",
        "\n",
        "# Generate word cloud for each decade\n",
        "print(\"\\nGenerating decade word clouds...\")\n",
        "for decade in sorted(decades.keys()):\n",
        "    if decades[decade]:\n",
        "        decade_text = ' '.join(decades[decade])\n",
        "\n",
        "        wordcloud = WordCloud(\n",
        "            width=1200,\n",
        "            height=600,\n",
        "            background_color='white',\n",
        "            stopwords=ALL_STOPWORDS,\n",
        "            max_words=100,\n",
        "            colormap='coolwarm'\n",
        "        ).generate(decade_text)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'President Oaks - {decade}', fontsize=18)\n",
        "        plt.axis('off')\n",
        "        plt.savefig(f'output/wordclouds/decade_{decade}.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"‚úì Generated word cloud for {decade}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcS6_gqyMzTo"
      },
      "source": [
        "## Step 8: Analyze by Talk Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCfFf60uMzTo"
      },
      "outputs": [],
      "source": [
        "# Group talks by type\n",
        "talk_types = defaultdict(list)\n",
        "\n",
        "for filepath in talk_files:\n",
        "    basename = os.path.basename(filepath)\n",
        "    parts = basename.split('_', 2)\n",
        "\n",
        "    if len(parts) >= 2:\n",
        "        talk_type = parts[1].replace('_', ' ')\n",
        "        content = processor.extract_talk_content(filepath)\n",
        "        talk_types[talk_type].append(content)\n",
        "\n",
        "print(f\"Found {len(talk_types)} talk types:\")\n",
        "for talk_type, texts in talk_types.items():\n",
        "    print(f\"  {talk_type}: {len(texts)} talks\")\n",
        "\n",
        "# Generate word cloud for each type\n",
        "print(\"\\nGenerating talk type word clouds...\")\n",
        "for talk_type, texts in talk_types.items():\n",
        "    if texts:\n",
        "        type_text = ' '.join(texts)\n",
        "\n",
        "        wordcloud = WordCloud(\n",
        "            width=1200,\n",
        "            height=600,\n",
        "            background_color='white',\n",
        "            stopwords=ALL_STOPWORDS,\n",
        "            max_words=100,\n",
        "            colormap='plasma'\n",
        "        ).generate(type_text)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'President Oaks - {talk_type}', fontsize=18)\n",
        "        plt.axis('off')\n",
        "        safe_type = talk_type.replace(' ', '_').lower()\n",
        "        plt.savefig(f'output/wordclouds/type_{safe_type}.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"‚úì Generated word cloud for {talk_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRYchdAhMzTo"
      },
      "source": [
        "## Step 9: Theme-Based Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUig-otPMzTo"
      },
      "outputs": [],
      "source": [
        "# Define themes\n",
        "themes = {\n",
        "    'Faith & Testimony': ['faith', 'testimony', 'believe', 'witness', 'know', 'truth', 'prayer'],\n",
        "    'Family & Marriage': ['family', 'marriage', 'children', 'parent', 'father', 'mother', 'home'],\n",
        "    'Service & Love': ['service', 'serve', 'love', 'charity', 'help', 'minister', 'compassion'],\n",
        "    'Covenant & Temple': ['covenant', 'temple', 'ordinance', 'baptism', 'endowment', 'sealing'],\n",
        "    'Scripture & Revelation': ['scripture', 'revelation', 'prophet', 'bible', 'book', 'mormon']\n",
        "}\n",
        "\n",
        "print(\"Generating theme-based word clouds...\\n\")\n",
        "\n",
        "for theme_name, keywords in themes.items():\n",
        "    print(f\"Processing theme: {theme_name}\")\n",
        "\n",
        "    # Extract sentences containing theme keywords\n",
        "    theme_sentences = []\n",
        "    for filepath in talk_files:\n",
        "        content = processor.extract_talk_content(filepath)\n",
        "        sentences = sent_tokenize(content)\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_lower = sentence.lower()\n",
        "            if any(keyword in sentence_lower for keyword in keywords):\n",
        "                theme_sentences.append(sentence)\n",
        "\n",
        "    if theme_sentences:\n",
        "        theme_text = ' '.join(theme_sentences)\n",
        "\n",
        "        wordcloud = WordCloud(\n",
        "            width=1200,\n",
        "            height=600,\n",
        "            background_color='white',\n",
        "            stopwords=ALL_STOPWORDS,\n",
        "            max_words=80,\n",
        "            colormap='viridis'\n",
        "        ).generate(theme_text)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.title(f'Theme: {theme_name}', fontsize=18)\n",
        "        plt.axis('off')\n",
        "        safe_theme = theme_name.replace(' & ', '_').lower()\n",
        "        plt.savefig(f'output/wordclouds/theme_{safe_theme}.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"‚úì Generated word cloud for {theme_name} ({len(theme_sentences)} sentences)\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_rinAeGMzTo"
      },
      "source": [
        "## Step 10: Download Results\\nRun this cell to create a ZIP file with all results that you can download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDIM-3EYMzTo"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create a ZIP file with all outputs\n",
        "print(\"Creating ZIP file with all results...\")\n",
        "\n",
        "zip_filename = 'oaks_analysis_results.zip'\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    # Add wordclouds\n",
        "    for root, dirs, files_list in os.walk('output/wordclouds'):\n",
        "        for file in files_list:\n",
        "            if file.endswith('.png'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, '.')\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "    # Add analysis files\n",
        "    for root, dirs, files_list in os.walk('output/analysis'):\n",
        "        for file in files_list:\n",
        "            if file.endswith(('.csv', '.json', '.txt')):\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, '.')\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "    # Add downloaded talks\n",
        "    for root, dirs, files_list in os.walk('data/talks'):\n",
        "        for file in files_list[:10]:  # Just include first 10 talks as sample\n",
        "            if file.endswith('.txt'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, '.')\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"‚úì Created {zip_filename}\")\n",
        "print(\"\\nContents:\")\n",
        "print(\"  - Word cloud images\")\n",
        "print(\"  - Word frequency analysis\")\n",
        "print(\"  - Sample talk texts\")\n",
        "print(\"\\nüì• Click below to download your results:\")\n",
        "\n",
        "# Trigger download\n",
        "files.download(zip_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjnwIE1tMzTp"
      },
      "source": [
        "## Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpawJhBdMzTp"
      },
      "outputs": [],
      "source": [
        "# Generate summary statistics\n",
        "print(\"=\"*60)\n",
        "print(\"ANALYSIS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Count statistics\n",
        "all_words = len(all_tokens)\n",
        "unique_words = len(set(all_tokens))\n",
        "num_talks = len(talk_files)\n",
        "\n",
        "print(f\"\\nüìä Overall Statistics:\")\n",
        "print(f\"  Total talks analyzed: {num_talks}\")\n",
        "print(f\"  Total words processed: {all_words:,}\")\n",
        "print(f\"  Unique words found: {unique_words:,}\")\n",
        "print(f\"  Average words per talk: {all_words // num_talks if num_talks > 0 else 0:,}\")\n",
        "\n",
        "print(f\"\\nüìÖ Temporal Coverage:\")\n",
        "for decade in sorted(decades.keys()):\n",
        "    print(f\"  {decade}: {len(decades[decade])} talks\")\n",
        "\n",
        "print(f\"\\nüìù Talk Types:\")\n",
        "for talk_type, texts in sorted(talk_types.items(), key=lambda x: len(x[1]), reverse=True):\n",
        "    print(f\"  {talk_type}: {len(texts)} talks\")\n",
        "\n",
        "print(f\"\\n‚úÖ Analysis Complete!\")\n",
        "print(f\"\\nAll visualizations have been saved to the 'output' folder.\")\n",
        "print(f\"Download the ZIP file above to get all your results.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8HulENLMzTp"
      },
      "source": [
        "## Optional: Advanced Topic Modeling with LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8dRkkNZMzTp"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "print(\"Performing topic modeling with LDA...\\n\")\n",
        "\n",
        "# Prepare documents\n",
        "documents = []\n",
        "for filepath in talk_files:\n",
        "    content = processor.extract_talk_content(filepath)\n",
        "    documents.append(content)\n",
        "\n",
        "# Create TF-IDF matrix\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=100,\n",
        "    min_df=2,\n",
        "    max_df=0.8,\n",
        "    stop_words=list(ALL_STOPWORDS),\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "doc_term_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# LDA model\n",
        "n_topics = 5\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_topics,\n",
        "    max_iter=10,\n",
        "    learning_method='online',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lda.fit(doc_term_matrix)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Display topics\n",
        "print(f\"Top {n_topics} Topics Discovered:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_words_idx = topic.argsort()[-10:][::-1]\n",
        "    top_words = [feature_names[i] for i in top_words_idx]\n",
        "    print(f\"\\nTopic {topic_idx + 1}:\")\n",
        "    print(\"  \", \", \".join(top_words))\n",
        "\n",
        "print(\"\\n‚úì Topic modeling complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv5kUdCTMzTq"
      },
      "source": [
        "---\\n## üéâ Congratulations!\\n\\nYou've successfully analyzed President Dallin H. Oaks' complete talk collection!\\n\\n### Next Steps:\\n1. **Download your results** using the ZIP file created above\\n2. **Explore the word clouds** to see patterns across decades and talk types\\n3. **Review the frequency analysis** for key themes\\n4. **Share your findings** or adapt this notebook for other speakers\\n\\n### To analyze ALL talks (not just the demo set):\\n- Remove `limit=50` from Step 4 to download the complete collection\\n- Note: This will take 30-60 minutes but gives you hundreds of talks\\n\\n### Educational Applications:\\nThis notebook demonstrates:\\n- Web scraping and data collection\\n- Natural Language Processing (NLP)\\n- Data visualization with word clouds\\n- Statistical text analysis\\n- Topic modeling with machine learning\\n\\nPerfect for teaching Python applications in business and operations management!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
